import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from config import TARGET_LENGTH
import joblib

def normalize_landmarks(landmarks):
    
    '''ä»¥è‚©è†€ä¸­å¿ƒç‚¹è¿›è¡Œå½’ä¸€åŒ–ï¼Œå»é™¤å™ªå£°'''
    landmarks = np.array(landmarks,dtype=np.float32)
    
    if len(landmarks != 96):
        if len(landmarks) < 96:
            landmarks = np.pad(landmarks,(0,96-len(landmarks)),'constant',constant_values=0.0)
        else:
            landmarks = landmarks[:96]
    
    landmarks = landmarks.reshape(48,2)
    
    left_shoulder_index = 42
    right_shoulder_index = 45
    
    left_shoulder = landmarks[left_shoulder_index]
    right_shoulder = landmarks[right_shoulder_index]
    
    if np.any(left_shoulder != 0) and np.any(right_shoulder != 0):
        center = (left_shoulder + right_shoulder) / 2
        scale = np.linalg.norm(right_shoulder - left_shoulder)
    elif np.any(left_shoulder != 0):
        # åªæœ‰å·¦è‚©æœ‰æ•ˆ
        center = left_shoulder
        scale = 1.0
    elif np.any(right_shoulder != 0):
        # åªæœ‰å³è‚©æœ‰æ•ˆ
        center = right_shoulder
        scale = 1.0
    else:
        non_zero = np.any(landmarks != 0 , axis= 1)
        if (np.any(non_zero)):
            center = np.mean(landmarks[non_zero],axis=0)
        else:
            center = np.array([0.5,0.5],dtype= np.float32)
        scale = 1.0
    
    normalized = landmarks - center
    if scale > 1e-6:
        normalized = normalized / scale
    
    result  = normalized.flatten()
    assert len(result) == 96, f"å½’ä¸€åŒ–åç»´åº¦é”™è¯¯: {len(result)}, æœŸæœ›96"
    
    return result


def adjust_sequence_length(sequence,target_length):
    
    sequence = np.array(sequence)
    current_length = len(sequence)
    
    if current_length == target_length:
        return sequence
    elif current_length > target_length:
        start = (current_length - target_length) // 2
        return sequence[start:start + target_length]
    else:
        total_padding_length = target_length - current_length
        
        left_padding_length = total_padding_length // 2
        right_padding_length = total_padding_length - left_padding_length
        
        if left_padding_length > 0:
            first_frame = sequence[0:1]
            left_pad = np.repeat(first_frame,left_padding_length,axis=0)
        else:
            left_pad = np.empty((0, sequence.shape[1]))
            
        if right_padding_length > 0:
            last_frame = sequence[-1:]
            right_pad = np.repeat(last_frame,right_padding_length,axis=0)
        else:
            right_pad = np.empty((0, sequence.shape[1]))
            
        result = np.vstack([left_pad,sequence,right_pad])
        
    assert len(result) == target_length, f"åºåˆ—é•¿åº¦è°ƒæ•´é”™è¯¯: {len(result)}, æœŸæœ›{target_length}"
    
    return result

def preprocess_data(X,y,target_length = TARGET_LENGTH):
    '''ä¸»å¤„ç†å‡½æ•°'''
    print(f"å¼€å§‹é¢„å¤„ç† {len(X)} ä¸ªåºåˆ—...")
    print(f"ç›®æ ‡åºåˆ—é•¿åº¦: {target_length}")
    
    processed_X = []
    valid_indices = []
    failed_count = 0
    
    for idx, sequence in enumerate(X):
        try:
            # è·³è¿‡ç©ºåºåˆ—
            if len(sequence) == 0:
                print(f"è­¦å‘Š: åºåˆ— {idx} ä¸ºç©ºï¼Œè·³è¿‡")
                failed_count += 1
                continue
            
            normalized_sequence = []
            for frame_id,frame in enumerate(sequence):
                try:
                    normolized_frame = normalize_landmarks(frame)
                    normalized_sequence.append(normolized_frame)
                except Exception as e:
                    print(f"è­¦å‘Š: åºåˆ— {idx} ç¬¬ {frame_id} å¸§å¤„ç†å¤±è´¥: {e}")
                    # ä½¿ç”¨é›¶å¸§ä½œä¸ºå…œåº•
                    normalized_sequence.append(np.zeros(96, dtype=np.float32))
                    
            normalized_sequence = np.array(normalized_sequence)
            processed_sequence = adjust_sequence_length(normalized_sequence,target_length)
            
            assert processed_sequence.shape == (target_length, 96), \
                f"åºåˆ— {idx} æœ€ç»ˆå½¢çŠ¶é”™è¯¯: {processed_sequence.shape}, æœŸæœ›({target_length}, 96)"
            
            processed_X.append(processed_sequence)
            valid_indices.append(idx)
        
        except Exception as e:
            print(f"é”™è¯¯: åºåˆ— {idx} å¤„ç†å®Œå…¨å¤±è´¥: {e}")
            failed_count += 1
            continue
        
        # è¿›åº¦æ˜¾ç¤º
        if (idx + 1) % 100 == 0:
            print(f"å·²å¤„ç†: {idx + 1}/{len(X)}, æˆåŠŸ: {len(processed_X)}, å¤±è´¥: {failed_count}")
    
    
    processed_X = np.array(processed_X,dtype=np.float32)
    processed_y = y[valid_indices]
    
    print(f"é¢„å¤„ç†å®Œæˆ:")
    print(f"  æˆåŠŸå¤„ç†: {len(processed_X)} ä¸ªåºåˆ—")
    print(f"  å¤±è´¥: {failed_count} ä¸ªåºåˆ—")
    print(f"  æœ€ç»ˆå½¢çŠ¶: X={processed_X.shape}, y={processed_y.shape}")
    
    # æœ€ç»ˆå½¢çŠ¶éªŒè¯
    expected_shape = (len(processed_X), target_length, 96)
    assert processed_X.shape == expected_shape, \
        f"æœ€ç»ˆXå½¢çŠ¶é”™è¯¯: {processed_X.shape}, æœŸæœ›{expected_shape}"
    
    return processed_X, processed_y


def split_and_normalize(X,y):
    """æ•°æ®åˆ’åˆ†å’Œæ ‡å‡†åŒ–"""
    print(f"å¼€å§‹æ•°æ®åˆ’åˆ†...")
    print(f"è¾“å…¥æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
    
    # æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒ
    unique_classes, class_counts = np.unique(y, return_counts=True)
    print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(zip(unique_classes, class_counts))}")
    
    
        
    X_train,X_test,y_train,y_test = train_test_split(
        X,y,test_size=0.1,random_state=42, stratify=y
    )
        
    
    print(f"æ•°æ®åˆ’åˆ†å®Œæˆ:")
    print(f"  è®­ç»ƒé›†: X={X_train.shape}, y={y_train.shape}")
    print(f"  æµ‹è¯•é›†: X={X_test.shape}, y={y_test.shape}")
    
    print("åº”ç”¨å…¨å±€æ ‡å‡†åŒ–...")
    scaler = StandardScaler()
    
    original_train_shape = X_train.shape
    original_test_shape = X_test.shape
    
    X_train_flat = X_train.reshape(-1,96)
    X_test_flat = X_test.reshape(-1,96)
    
    X_train_norm_flat = scaler.fit_transform(X_train_flat)
    X_test_norm_flat = scaler.transform(X_test_flat)
    
    X_train_norm = X_train_norm_flat.reshape(original_train_shape)
    X_test_norm = X_test_norm_flat.reshape(original_test_shape)
    
    print("æ ‡å‡†åŒ–å®Œæˆ")
    
    return (X_train_norm,X_test_norm), (y_train,y_test), scaler


if __name__ == "__main__":
    print("æ‰‹è¯­è¯†åˆ«æ•°æ®é¢„å¤„ç†")
    print("=" * 50)
    
    try:
        # åŠ è½½åŸå§‹æ•°æ®
        print("åŠ è½½åŸå§‹æ•°æ®...")
        X = np.load('X_features.npy', allow_pickle=True)
        y = np.load('y_labels.npy')
        print(f"åŸå§‹æ•°æ®: XåŒ…å«{len(X)}ä¸ªåºåˆ—, yå½¢çŠ¶{y.shape}")
        
        # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
        if len(X) != len(y):
            raise ValueError(f"ç‰¹å¾å’Œæ ‡ç­¾æ•°é‡ä¸åŒ¹é…: {len(X)} vs {len(y)}")
        
        # ä¸»è¦é¢„å¤„ç†
        X_processed, y_processed = preprocess_data(X, y, target_length=TARGET_LENGTH)
        
        # æ•°æ®åˆ’åˆ†å’Œæ ‡å‡†åŒ–
        (X_train,  X_test), (y_train, y_test), scaler = split_and_normalize(X_processed, y_processed)
        
        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        print("\nä¿å­˜å¤„ç†åçš„æ•°æ®...")
        np.save('X_train.npy', X_train)
        np.save('X_test.npy', X_test)
        np.save('y_train.npy', y_train)
        np.save('y_test.npy', y_test)
        joblib.dump(scaler, 'feature_scaler.pkl')
        
        # æœ€ç»ˆæŠ¥å‘Š
        print("\nâœ… æ•°æ®é¢„å¤„ç†å®Œæˆ!")
        print(f"ğŸ“Š æœ€ç»ˆæ•°æ®é›†:")
        print(f"   è®­ç»ƒé›†: {X_train.shape} -> æ¯ä¸ªè§†é¢‘ {TARGET_LENGTH}Ã—96")
        print(f"   æµ‹è¯•é›†: {X_test.shape} -> æ¯ä¸ªè§†é¢‘ {TARGET_LENGTH}Ã—96")
        print(f"ğŸ’¾ ä¿å­˜çš„æ–‡ä»¶:")
        print(f"   - X_train.npy, X_test.npy")
        print(f"   - y_train.npy, y_test.npy")
        print(f"   - feature_scaler.pkl")
        
    except FileNotFoundError as e:
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {e}")
        print("è¯·ç¡®ä¿å·²è¿è¡Œç‰¹å¾æå–è„šæœ¬ç”Ÿæˆ X_features.npy å’Œ y_labels.npy")
        
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
        